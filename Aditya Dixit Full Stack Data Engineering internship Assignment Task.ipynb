{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Libraries.\n",
    "2. Importing the Input Data.\n",
    "3. Scraping the required Data.\n",
    "4. (Optional) Storing the data in json for later purposes.\n",
    "5. Cleaning the Data.\n",
    "- 5.1 Importing the Stop_Words files.\n",
    "- 5.2 Delete the stop words.\n",
    "- 5.3 Finding positive words & negative words.\n",
    "- 5.4 Importing the Positive and Negative words Dictionaries and adding the new positive word and negative words.\n",
    "6. Calculating the required Variables.\n",
    "7. Creating the Output Data Structure - Output Data Structures.xlsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Aditya\n",
      "[nltk_data]     Dixit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aditya\n",
      "[nltk_data]     Dixit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Dixit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from scrapy.http import HtmlResponse\n",
    "import json \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import syllapy\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing the Input Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(r\"C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\Input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>931 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL_ID                                                URL\n",
       "0    blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1    blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2    blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3    blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4    blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..               ...                                                ...\n",
       "926              NaN                                                NaN\n",
       "927              NaN                                                NaN\n",
       "928              NaN                                                NaN\n",
       "929              NaN                                                NaN\n",
       "930              NaN                                                NaN\n",
       "\n",
       "[931 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = input_df.dropna()\n",
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping the required Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Extractor - method to scrape required data from a given URL\n",
    "\n",
    "def Content_extractor(link):\n",
    "    content = {\n",
    "        'URL' : link,\n",
    "        'Body': \"\"\n",
    "        }\n",
    "    \n",
    "    body = []\n",
    "    \n",
    "    req = requests.get(link)\n",
    "    resp = HtmlResponse('example.com', body=req.text, encoding='utf-8')\n",
    "\n",
    "    try:\n",
    "        if req.status_code == 200:\n",
    "            # 1\n",
    "            content_title = resp.xpath(\"//head//title//text()\").get()\n",
    "            body.append(content_title.strip())\n",
    "\n",
    "            # 2 & 3\n",
    "            content_body1 = resp.xpath(\"//div[@class='td-container']//div[@class='td-ss-main-content']//div[@class='td-post-content tagdiv-type']//text()\").getall()\n",
    "            content_body2 = resp.xpath(\"(//body//div[@class='tdb-block-inner td-fix-index'])[15]//text()\").getall()\n",
    "            body.extend(content_body1)\n",
    "            body.extend(content_body2)\n",
    "            body = [b.strip() for b in body if b.strip()]\n",
    "            content['Body'] = \" \".join(body)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting content from {link}: {e}\")  \n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "content_list = []\n",
    "\n",
    "for url in input_df['URL']:\n",
    "    \n",
    "    try:\n",
    "        execute = Content_extractor(url)\n",
    "        content_list.append(execute)\n",
    "    except:\n",
    "        print(url)\n",
    "\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. (Optional) Storing the data in json for later purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to content_list.json\n"
     ]
    }
   ],
   "source": [
    "# file_path = 'content_list.json'\n",
    "\n",
    "# with open(file_path, 'w') as json_file:\n",
    "#     json.dump(content_list, json_file, indent=4)\n",
    "\n",
    "# print(f\"Data has been saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.1 Importing the Stop_Words files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14238\n",
      "12839\n"
     ]
    }
   ],
   "source": [
    "file_path1 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_Auditor.txt'\n",
    "with open(file_path1, 'r') as file:\n",
    "    auditor_stop = file.read()\n",
    "auditor_stop_list = auditor_stop.split()\n",
    "auditor_stop_list = [a.lower() for a in auditor_stop_list]\n",
    "\n",
    "file_path2 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_Currencies.txt'\n",
    "with open(file_path2, 'r', encoding= 'latin-1') as file:\n",
    "    currencies_stop = file.read()\n",
    "currencies_stop_list = currencies_stop.split()\n",
    "currencies_stop_list = [i for i in currencies_stop_list if i != '|']\n",
    "currencies_stop_list = [a.lower() for a in currencies_stop_list]\n",
    "\n",
    "file_path3 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_DatesandNumbers.txt'\n",
    "with open(file_path3, 'r') as file:\n",
    "    datesandnumbers_stop = file.read()\n",
    "\n",
    "datesandnumbers_stop_list = datesandnumbers_stop.split()\n",
    "datesandnumbers_stop_list = [i for i in datesandnumbers_stop_list if i != '|']\n",
    "datesandnumbers_stop_list = [a.lower() for a in datesandnumbers_stop_list]\n",
    "\n",
    "file_path4 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_Generic.txt'\n",
    "with open(file_path4, 'r') as file:\n",
    "    generic_stop = file.read()\n",
    "generics_stop_list = generic_stop.split()\n",
    "generics_stop_list = [i for i in generics_stop_list if i != '|']\n",
    "generics_stop_list = [a.lower() for a in generics_stop_list]\n",
    "\n",
    "file_path5 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_GenericLong.txt'\n",
    "with open(file_path5, 'r') as file:\n",
    "    generic_long_stop = file.read()\n",
    "generic_long_stop_list = generic_long_stop.split()\n",
    "generic_long_stop_list = [i for i in generic_long_stop_list if i != '|']\n",
    "generic_long_stop_list = [a.lower() for a in generic_long_stop_list]\n",
    "\n",
    "file_path6 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_Geographic.txt'\n",
    "with open(file_path6, 'r') as file:\n",
    "    geographic_stop = file.read()\n",
    "geographic_stop_list = geographic_stop.split()\n",
    "geographic_stop_list = [i for i in geographic_stop_list if i != '|']\n",
    "geographic_stop_list = [a.lower() for a in geographic_stop_list]\n",
    "\n",
    "file_path7 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\StopWords\\StopWords_Names.txt'\n",
    "with open(file_path7, 'r') as file:\n",
    "    names_stop = file.read()\n",
    "names_stop_list = names_stop.split()\n",
    "names_stop_list = [i for i in names_stop_list if i != '|']\n",
    "names_stop_list = [a.lower() for a in names_stop_list]\n",
    "\n",
    "stop_words = auditor_stop_list+currencies_stop_list+datesandnumbers_stop_list+generic_long_stop_list+generics_stop_list+geographic_stop_list+names_stop_list\n",
    "print(len(stop_words))\n",
    "stop_words=list(set(stop_words))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.2 Delete the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in content_list:\n",
    "    c['Stop_word_removed'] = \"\"\n",
    "\n",
    "    body = c['Body'].lower().split()\n",
    "    mod_body = []\n",
    "\n",
    "    for h in body:\n",
    "        if h.strip() not in stop_words:\n",
    "            mod_body.append(h.strip())\n",
    "\n",
    "    c['Stop_word_removed'] = mod_body   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.3 Finding positive words & negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "posi = []\n",
    "nega = []\n",
    "neutral = []\n",
    "\n",
    "for c in content_list:\n",
    "\n",
    "    for i in c['Stop_word_removed']:\n",
    "        sentiment_score= sid.polarity_scores(i)\n",
    "        # print(i,sentiment_score['compound'])\n",
    "        if sentiment_score['compound'] >= 0.05:\n",
    "            posi.append(i)\n",
    "        elif sentiment_score['compound'] <= -0.05:\n",
    "            nega.append(i)\n",
    "        else:\n",
    "            neutral.append(i)\n",
    "            # print(c,i)\n",
    "\n",
    "posi = list(set(posi))\n",
    "len(posi)\n",
    "\n",
    "nega = list(set(nega))\n",
    "len(nega)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.4 Importing the Positive and Negative words Dictionaries and adding the new positive word and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4783\n"
     ]
    }
   ],
   "source": [
    "file_path8 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\MasterDictionary\\positive-words.txt'\n",
    "with open(file_path8, 'r') as file:\n",
    "    positive_words = file.read()\n",
    "\n",
    "positive_words_list = positive_words.split()\n",
    "positive_words_list = [i for i in positive_words_list if i != '|']\n",
    "positive_words_list = [a.lower() for a in positive_words_list]\n",
    "positive_words_list.extend(posi)\n",
    "positive_words_list = list(set(positive_words_list))\n",
    "\n",
    "file_path9 = r'C:\\Users\\Aditya Dixit\\Desktop\\Writing works\\PYTHON PRACTICE\\Aditya Dixit Data Engineering Assignment\\20211030 Test Assignment-20240227T125146Z-001\\20211030 Test Assignment\\MasterDictionary\\negative-words.txt'\n",
    "with open(file_path9, 'r', encoding= 'latin-1') as file:\n",
    "    negative_words = file.read()\n",
    "\n",
    "negative_words_list = negative_words.split()\n",
    "print(len(negative_words_list))\n",
    "negative_words_list = [i for i in negative_words_list if i != '|']\n",
    "negative_words_list = [a.lower() for a in negative_words_list]\n",
    "negative_words_list.extend(nega)\n",
    "negative_words_list = list(set(negative_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calculating the required Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL': 'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/',\n",
       " 'Body': 'Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040. - Blackcoffer Insights We have seen a huge development and dependence of people on technology in recent years. We have also seen the development of AI and ChatGPT in recent years. So it is a normal thing that we will become fully dependent on technology by 2040. Information technology will be a major power for all the developing nations. As a member of a developing nation, India is rapidly growing its IT base. It has also grown some IT cities which will be the major control centres for Information technology by 2040. Rising IT cities Noida:- Noida in Uttar Pradesh near New Delhi is an emerging IT sector now. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Noida has a market base of billions of dollars and is doing a great job of boosting the national economy. The establishment of so many software companies has made Noida an information technology hub. Gurgaon:- Gurgaon in Haryana is also an emerging IT hub. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Gurgaon has a market base of billions of dollars and is doing a great job of boosting the national economy. Bengaluru:- Bengaluru is called as the IT hub of India. It is also a smart city. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Bengaluru has a market base of billions of dollars and is doing a great job of boosting the national economy. Kolkata:- Kolkata in West Bengal is an emerging major IT hub. The new Kolkata i.e. Saltlake Sector\\xa0 5, New town, Rajarhat area of Kolkata is a major IT hub. The government is giving the software companies land at almost free of cost to set up the companies there. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Kolkata has a market base of billions of dollars and is doing a great job of boosting the national economy. Impact on Economy There is a huge impact of the rising IT cities on our economy. Some of the effects are- Demand:- The rising IT cities will greatly help to boost our economy. These will create a huge demand for raw materials. The products when ready will be a huge demand for the people too. Supply:- – Supply means the fulfilment of demand. In a large and highly populous country like India, there is always a demand for finished products. If more IT cities do not develop, the companies cannot fulfil the needs and desires of the people of a populous country like India. As IT cities develop, more IT companies will come, which will supply more and more finished IT products to our people. Market: A market is a place where different economic agents like buyers and sellers interact with one another. In a populous country like India, there is a huge market. As IT cities will grow, more and more IT companies will come from across the world and more will the competition in the market increase. This will help consumers as they will get more and more differentiated products and the market will also run smoothly. A competitive market is always good and healthy. We can safely assume that our oligopoly market will surely tend to reach a perfectly competitive market by the year 2040. Revenue:- As the market increases, more revenue will be generated. Now at present, the IT revenue of India is 245 million dollars, 19 million dollars more than the financial year 2022. If IT cities grow, then more companies will invest which leads to an increase in the IT market which in turn generates more revenue in India. We can expect that the IT revenue of India will cross or nearly tend to reach 10 billion dollars by 2040. Impact on Environment The rising IT cities will create a huge impact on the environment, the maximum of which will be harmful effects. The impact of rising IT cities on the environment is- Deforestation:- There will be cutting of trees in huge numbers to make the building of the IT companies which will cause great harm to the environment. The cutting of trees on a large scale will also cause mass degradation of forests. More carbon footprint:- The IT companies will generate more carbon footprint in the atmosphere. South Asian countries including India are known for their lower carbon footprint. But if the IT sector grows this way then we will also be at the same pace of generation of carbon footprint by 2040. Death of birds:- The cell phone and mobile towers by the telecom companies caused the death of birds which caused a great imbalance in the ecosystem. The number of sparrows has been reduced due to this phenomenon. If this goes on we can see the extinction of many bird species by 2040. Impact on infrastructure There are many contributions of the IT cities on infrastructure.\\xa0 They are- Transportation:- The rising IT cities need an excellent transport system for the supply of raw materials and delivery of the finished products into the market. So the transportation system develops in that area. So we have an excellent transport system by 2040. Need for a public transport system:- There is a need for a public transport system in the IT cities. As the IT cities are a source of employment and a huge population reside in these areas, there is an adequate need for public transport systems like buses, taxis etc. We hope that it will be improved by 2040. Water supply:- As a huge number of people reside in the IT cities there is a need for adequate water supply to fulfil the needs of people as well as for industries. This will help us to find many new methods of water supply and conservation by 2040. Electricity:- Electric supply is the lifeline of the sector. Without an electric supply, no machines will run and not even the IT cities will flourish. If the IT cities flourish this way, we going to have an excellent electric supply by 2040. Healthcare:- As a large number of people reside in IT cities, there is a need for proper health infrastructure and healthcare facilities for the people. So with the growth of IT cities, our healthcare system will also improve by 2040. Education:- Education is the primary key or core of any nation. There must be proper education and training centres in those IT cities to fulfil the people’s demands.\\xa0 So with the growth of IT cities, the education system will also develop by 2040. Our education is also going to be skill-oriented. Impact on city life With the growth of IT cities, more people will get jobs and will earn more. So the purchasing power of the people will increase. People will lead a better lifestyle. They will buy things of good brand value. The tastes and preferences of people will also change. The human development index is going to increase. People will buy good quality food and good quality cars. So the food, automobile and many other industries are going to increase. So there will be a huge impact on city life by 2040. Blackcoffer Insights 47: Arka Mukhopadhyay, West Bengal University Of Animal And Fishery Sciences',\n",
       " 'Stop_word_removed': ['rising',\n",
       "  'impact',\n",
       "  'economy,',\n",
       "  'environment,',\n",
       "  'infrastructure,',\n",
       "  'life',\n",
       "  '2040.',\n",
       "  '-',\n",
       "  'blackcoffer',\n",
       "  'insights',\n",
       "  'huge',\n",
       "  'development',\n",
       "  'dependence',\n",
       "  'people',\n",
       "  'technology',\n",
       "  'recent',\n",
       "  'years.',\n",
       "  'development',\n",
       "  'chatgpt',\n",
       "  'recent',\n",
       "  'years.',\n",
       "  'normal',\n",
       "  'thing',\n",
       "  'fully',\n",
       "  'dependent',\n",
       "  'technology',\n",
       "  '2040.',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'developing',\n",
       "  'nations.',\n",
       "  'member',\n",
       "  'developing',\n",
       "  'nation,',\n",
       "  'rapidly',\n",
       "  'growing',\n",
       "  'base.',\n",
       "  'grown',\n",
       "  'control',\n",
       "  'centres',\n",
       "  'information',\n",
       "  'technology',\n",
       "  '2040.',\n",
       "  'rising',\n",
       "  'noida:-',\n",
       "  'noida',\n",
       "  'uttar',\n",
       "  'pradesh',\n",
       "  'emerging',\n",
       "  'sector',\n",
       "  'now.',\n",
       "  'companies',\n",
       "  'google,',\n",
       "  'microsoft,',\n",
       "  'ibm,',\n",
       "  'infosys',\n",
       "  'set',\n",
       "  'companies',\n",
       "  'here.',\n",
       "  'noida',\n",
       "  'market',\n",
       "  'base',\n",
       "  'billions',\n",
       "  'dollars',\n",
       "  'great',\n",
       "  'job',\n",
       "  'boosting',\n",
       "  'national',\n",
       "  'economy.',\n",
       "  'establishment',\n",
       "  'software',\n",
       "  'companies',\n",
       "  'made',\n",
       "  'noida',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'hub.',\n",
       "  'gurgaon:-',\n",
       "  'gurgaon',\n",
       "  'haryana',\n",
       "  'emerging',\n",
       "  'hub.',\n",
       "  'companies',\n",
       "  'google,',\n",
       "  'microsoft,',\n",
       "  'ibm,',\n",
       "  'infosys',\n",
       "  'set',\n",
       "  'companies',\n",
       "  'here.',\n",
       "  'gurgaon',\n",
       "  'market',\n",
       "  'base',\n",
       "  'billions',\n",
       "  'dollars',\n",
       "  'great',\n",
       "  'job',\n",
       "  'boosting',\n",
       "  'national',\n",
       "  'economy.',\n",
       "  'bengaluru:-',\n",
       "  'bengaluru',\n",
       "  'called',\n",
       "  'hub',\n",
       "  'india.',\n",
       "  'city.',\n",
       "  'companies',\n",
       "  'google,',\n",
       "  'microsoft,',\n",
       "  'ibm,',\n",
       "  'infosys',\n",
       "  'set',\n",
       "  'companies',\n",
       "  'here.',\n",
       "  'bengaluru',\n",
       "  'market',\n",
       "  'base',\n",
       "  'billions',\n",
       "  'dollars',\n",
       "  'great',\n",
       "  'job',\n",
       "  'boosting',\n",
       "  'national',\n",
       "  'economy.',\n",
       "  'kolkata:-',\n",
       "  'kolkata',\n",
       "  'bengal',\n",
       "  'emerging',\n",
       "  'hub.',\n",
       "  'kolkata',\n",
       "  'i.e.',\n",
       "  'saltlake',\n",
       "  'sector',\n",
       "  '5,',\n",
       "  'town,',\n",
       "  'rajarhat',\n",
       "  'area',\n",
       "  'kolkata',\n",
       "  'hub.',\n",
       "  'government',\n",
       "  'giving',\n",
       "  'software',\n",
       "  'companies',\n",
       "  'cost',\n",
       "  'set',\n",
       "  'companies',\n",
       "  'there.',\n",
       "  'companies',\n",
       "  'google,',\n",
       "  'microsoft,',\n",
       "  'ibm,',\n",
       "  'infosys',\n",
       "  'set',\n",
       "  'companies',\n",
       "  'here.',\n",
       "  'kolkata',\n",
       "  'market',\n",
       "  'base',\n",
       "  'billions',\n",
       "  'dollars',\n",
       "  'great',\n",
       "  'job',\n",
       "  'boosting',\n",
       "  'national',\n",
       "  'economy.',\n",
       "  'impact',\n",
       "  'economy',\n",
       "  'huge',\n",
       "  'impact',\n",
       "  'rising',\n",
       "  'economy.',\n",
       "  'effects',\n",
       "  'are-',\n",
       "  'demand:-',\n",
       "  'rising',\n",
       "  'greatly',\n",
       "  'boost',\n",
       "  'economy.',\n",
       "  'create',\n",
       "  'huge',\n",
       "  'demand',\n",
       "  'raw',\n",
       "  'materials.',\n",
       "  'products',\n",
       "  'huge',\n",
       "  'demand',\n",
       "  'people',\n",
       "  'too.',\n",
       "  'supply:-',\n",
       "  '–',\n",
       "  'supply',\n",
       "  'fulfilment',\n",
       "  'demand.',\n",
       "  'highly',\n",
       "  'populous',\n",
       "  'india,',\n",
       "  'demand',\n",
       "  'finished',\n",
       "  'products.',\n",
       "  'develop,',\n",
       "  'companies',\n",
       "  'fulfil',\n",
       "  'desires',\n",
       "  'people',\n",
       "  'populous',\n",
       "  'india.',\n",
       "  'develop,',\n",
       "  'companies',\n",
       "  'come,',\n",
       "  'supply',\n",
       "  'finished',\n",
       "  'products',\n",
       "  'people.',\n",
       "  'market:',\n",
       "  'market',\n",
       "  'economic',\n",
       "  'agents',\n",
       "  'buyers',\n",
       "  'interact',\n",
       "  'another.',\n",
       "  'populous',\n",
       "  'india,',\n",
       "  'huge',\n",
       "  'market.',\n",
       "  'grow,',\n",
       "  'companies',\n",
       "  'world',\n",
       "  'competition',\n",
       "  'market',\n",
       "  'increase.',\n",
       "  'consumers',\n",
       "  'differentiated',\n",
       "  'products',\n",
       "  'market',\n",
       "  'run',\n",
       "  'smoothly.',\n",
       "  'competitive',\n",
       "  'market',\n",
       "  'healthy.',\n",
       "  'safely',\n",
       "  'assume',\n",
       "  'oligopoly',\n",
       "  'market',\n",
       "  'surely',\n",
       "  'tend',\n",
       "  'reach',\n",
       "  'perfectly',\n",
       "  'competitive',\n",
       "  'market',\n",
       "  '2040.',\n",
       "  'revenue:-',\n",
       "  'market',\n",
       "  'increases,',\n",
       "  'revenue',\n",
       "  'generated.',\n",
       "  'present,',\n",
       "  'revenue',\n",
       "  '245',\n",
       "  'dollars,',\n",
       "  '19',\n",
       "  'dollars',\n",
       "  'financial',\n",
       "  '2022.',\n",
       "  'grow,',\n",
       "  'companies',\n",
       "  'invest',\n",
       "  'leads',\n",
       "  'increase',\n",
       "  'market',\n",
       "  'turn',\n",
       "  'generates',\n",
       "  'revenue',\n",
       "  'india.',\n",
       "  'expect',\n",
       "  'revenue',\n",
       "  'tend',\n",
       "  'reach',\n",
       "  '10',\n",
       "  'dollars',\n",
       "  '2040.',\n",
       "  'impact',\n",
       "  'environment',\n",
       "  'rising',\n",
       "  'create',\n",
       "  'huge',\n",
       "  'impact',\n",
       "  'environment,',\n",
       "  'maximum',\n",
       "  'harmful',\n",
       "  'effects.',\n",
       "  'impact',\n",
       "  'rising',\n",
       "  'environment',\n",
       "  'is-',\n",
       "  'deforestation:-',\n",
       "  'cutting',\n",
       "  'trees',\n",
       "  'huge',\n",
       "  'make',\n",
       "  'building',\n",
       "  'companies',\n",
       "  'great',\n",
       "  'harm',\n",
       "  'environment.',\n",
       "  'cutting',\n",
       "  'trees',\n",
       "  'scale',\n",
       "  'mass',\n",
       "  'degradation',\n",
       "  'forests.',\n",
       "  'carbon',\n",
       "  'footprint:-',\n",
       "  'companies',\n",
       "  'generate',\n",
       "  'carbon',\n",
       "  'footprint',\n",
       "  'atmosphere.',\n",
       "  'asian',\n",
       "  'including',\n",
       "  'carbon',\n",
       "  'footprint.',\n",
       "  'sector',\n",
       "  'grows',\n",
       "  'generation',\n",
       "  'carbon',\n",
       "  'footprint',\n",
       "  '2040.',\n",
       "  'death',\n",
       "  'birds:-',\n",
       "  'cell',\n",
       "  'phone',\n",
       "  'mobile',\n",
       "  'towers',\n",
       "  'telecom',\n",
       "  'companies',\n",
       "  'caused',\n",
       "  'death',\n",
       "  'birds',\n",
       "  'caused',\n",
       "  'great',\n",
       "  'imbalance',\n",
       "  'ecosystem.',\n",
       "  'number',\n",
       "  'sparrows',\n",
       "  'reduced',\n",
       "  'due',\n",
       "  'phenomenon.',\n",
       "  'extinction',\n",
       "  'species',\n",
       "  '2040.',\n",
       "  'impact',\n",
       "  'infrastructure',\n",
       "  'contributions',\n",
       "  'infrastructure.',\n",
       "  'are-',\n",
       "  'transportation:-',\n",
       "  'rising',\n",
       "  'excellent',\n",
       "  'transport',\n",
       "  'system',\n",
       "  'supply',\n",
       "  'raw',\n",
       "  'materials',\n",
       "  'delivery',\n",
       "  'finished',\n",
       "  'products',\n",
       "  'market.',\n",
       "  'transportation',\n",
       "  'system',\n",
       "  'develops',\n",
       "  'area.',\n",
       "  'excellent',\n",
       "  'transport',\n",
       "  'system',\n",
       "  '2040.',\n",
       "  'public',\n",
       "  'transport',\n",
       "  'system:-',\n",
       "  'public',\n",
       "  'transport',\n",
       "  'system',\n",
       "  'cities.',\n",
       "  'source',\n",
       "  'employment',\n",
       "  'huge',\n",
       "  'population',\n",
       "  'reside',\n",
       "  'areas,',\n",
       "  'adequate',\n",
       "  'public',\n",
       "  'transport',\n",
       "  'systems',\n",
       "  'buses,',\n",
       "  'taxis',\n",
       "  'etc.',\n",
       "  'improved',\n",
       "  '2040.',\n",
       "  'water',\n",
       "  'supply:-',\n",
       "  'huge',\n",
       "  'number',\n",
       "  'people',\n",
       "  'reside',\n",
       "  'adequate',\n",
       "  'water',\n",
       "  'supply',\n",
       "  'fulfil',\n",
       "  'people',\n",
       "  'industries.',\n",
       "  'find',\n",
       "  'methods',\n",
       "  'water',\n",
       "  'supply',\n",
       "  'conservation',\n",
       "  '2040.',\n",
       "  'electricity:-',\n",
       "  'electric',\n",
       "  'supply',\n",
       "  'lifeline',\n",
       "  'sector.',\n",
       "  'electric',\n",
       "  'supply,',\n",
       "  'machines',\n",
       "  'run',\n",
       "  'flourish.',\n",
       "  'flourish',\n",
       "  'way,',\n",
       "  'excellent',\n",
       "  'electric',\n",
       "  'supply',\n",
       "  '2040.',\n",
       "  'healthcare:-',\n",
       "  'number',\n",
       "  'people',\n",
       "  'reside',\n",
       "  'cities,',\n",
       "  'proper',\n",
       "  'health',\n",
       "  'infrastructure',\n",
       "  'healthcare',\n",
       "  'facilities',\n",
       "  'people.',\n",
       "  'growth',\n",
       "  'cities,',\n",
       "  'healthcare',\n",
       "  'system',\n",
       "  'improve',\n",
       "  '2040.',\n",
       "  'education:-',\n",
       "  'education',\n",
       "  'primary',\n",
       "  'nation.',\n",
       "  'proper',\n",
       "  'education',\n",
       "  'training',\n",
       "  'centres',\n",
       "  'fulfil',\n",
       "  'people’s',\n",
       "  'demands.',\n",
       "  'growth',\n",
       "  'cities,',\n",
       "  'education',\n",
       "  'system',\n",
       "  'develop',\n",
       "  '2040.',\n",
       "  'education',\n",
       "  'skill-oriented.',\n",
       "  'impact',\n",
       "  'life',\n",
       "  'growth',\n",
       "  'cities,',\n",
       "  'people',\n",
       "  'jobs',\n",
       "  'earn',\n",
       "  'more.',\n",
       "  'purchasing',\n",
       "  'people',\n",
       "  'increase.',\n",
       "  'people',\n",
       "  'lead',\n",
       "  'lifestyle.',\n",
       "  'buy',\n",
       "  'things',\n",
       "  'value.',\n",
       "  'tastes',\n",
       "  'preferences',\n",
       "  'people',\n",
       "  'change.',\n",
       "  'human',\n",
       "  'development',\n",
       "  'index',\n",
       "  'increase.',\n",
       "  'people',\n",
       "  'buy',\n",
       "  'quality',\n",
       "  'food',\n",
       "  'quality',\n",
       "  'cars.',\n",
       "  'food,',\n",
       "  'automobile',\n",
       "  'industries',\n",
       "  'increase.',\n",
       "  'huge',\n",
       "  'impact',\n",
       "  'life',\n",
       "  '2040.',\n",
       "  'blackcoffer',\n",
       "  'insights',\n",
       "  '47:',\n",
       "  'arka',\n",
       "  'mukhopadhyay,',\n",
       "  'bengal',\n",
       "  'university',\n",
       "  'animal',\n",
       "  'fishery',\n",
       "  'sciences'],\n",
       " 'Positive Score': 55,\n",
       " 'Negative Score': 12,\n",
       " 'Polarity Score': 0.6417910351971488,\n",
       " 'Subjectivity Score': 0.12984496098867257,\n",
       " 'Average Sentence Length': 17,\n",
       " 'Percentage of complex words': 0.13596138374899436,\n",
       " 'Fog index': 6.854384553499599,\n",
       " 'Average Number of Words Per ': 17.68354430379747,\n",
       " 'Word Count': 480,\n",
       " 'Syllable Count per word': 0.2641801548205489,\n",
       " 'Personal Pronouns': 11,\n",
       " 'Average Word Length': 4.714400643604184,\n",
       " 'Complex Word Count': 169}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_syllables(word):\n",
    "    \n",
    "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        return max(1, syllapy.count(word) - 1)\n",
    "    else:\n",
    "        return syllapy.count(word)\n",
    "\n",
    "for c in content_list:\n",
    "\n",
    "    c['Positive Score']=0\n",
    "    c['Negative Score']=0\n",
    "    c['Polarity Score']=0\n",
    "    c['Subjectivity Score']=0\n",
    "    c['Average Sentence Length']=0\n",
    "    c['Average Sentence Length']=0\n",
    "    c['Percentage of complex words']=0\n",
    "    c['Fog index']=0\n",
    "    c['Average Number of Words Per ']=0\n",
    "    c['Word Count']=0    \n",
    "    c['Syllable Count per word']=0\n",
    "    c['Personal Pronouns']=0\n",
    "    c['Average Word Length']=0\n",
    "    c['Complex Word Count']=0\n",
    "\n",
    "    for g in c['Stop_word_removed']:\n",
    "\n",
    "        if g in positive_words_list:\n",
    "            c['Positive Score']+=1\n",
    "        elif g in negative_words_list:\n",
    "            c['Negative Score']+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    c['Polarity Score'] = (c['Positive Score']-c['Negative Score'])/(c['Positive Score'] + c['Negative Score'] + 0.000001)\n",
    "\n",
    "    c['Subjectivity Score'] = (c['Positive Score'] + c['Negative Score'])/(len(c['Stop_word_removed'])+0.000001)\n",
    "\n",
    "    words_tokens = word_tokenize(c['Body'])\n",
    "    sentence_token = sent_tokenize(c['Body'])\n",
    "\n",
    "    if len(sentence_token) != 0:\n",
    "        c['Average Sentence Length'] = int(len(words_tokens)/len(sentence_token))\n",
    "\n",
    "    complex_words = []\n",
    "    for w in c['Body'].split():\n",
    "        if syllapy.count(w) > 2:\n",
    "            complex_words.append(w)\n",
    "\n",
    "    c['Complex Word Count'] = len(complex_words)\n",
    "\n",
    "    if len(c['Body'].split()) !=0:\n",
    "        c['Percentage of complex words'] = len(complex_words)/len(c['Body'].split())\n",
    "        c['Fog index'] = (0.4)*((c['Average Sentence Length'])+(c['Percentage of complex words']))\n",
    "    \n",
    "    if len(sentence_token) != 0:\n",
    "        c['Average Number of Words Per '] = len(words_tokens)/len(sentence_token)\n",
    "\n",
    "    words = nltk.word_tokenize(c['Body'])\n",
    "    stop_words1 = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    c['Word Count']=len(cleaned_words)\n",
    "\n",
    "    syllable_count=[]\n",
    "    for w in c['Body'].split():\n",
    "        syllable_count.append(count_syllables(w))\n",
    "\n",
    "    if len(c['Body'].split()) != 0:\n",
    "        c['Syllable Count per word'] = (sum(syllable_count))/(len(c['Body']))\n",
    "\n",
    "    pronouns_pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "    personal_pronouns = re.findall(pronouns_pattern, c['Body'], flags=re.IGNORECASE)\n",
    "    personal_pronouns = [pronoun for pronoun in personal_pronouns if pronoun.lower() != 'us']\n",
    "    c['Personal Pronouns'] = len(personal_pronouns)\n",
    "\n",
    "    words = c['Body'].split()\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    if len(c['Body']) !=0:\n",
    "        c['Average Word Length']= total_characters / total_words   \n",
    "    \n",
    "    \n",
    "\n",
    "content_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Creating the Output Data Structure - Output Data Structures.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of URL_ID: 100\n",
      "Length of URL: 100\n",
      "Length of POSITIVE SCORE: 100\n",
      "Length of NEGATIVE SCORE: 100\n",
      "Length of POLARITY SCORE: 100\n",
      "Length of SUBJECTIVITY SCORE: 100\n",
      "Length of AVG SENTENCE LENGTH: 100\n",
      "Length of PERCENTAGE OF COMPLEX WORDS: 100\n",
      "Length of FOG INDEX: 100\n",
      "Length of AVG NUMBER OF WORDS PER SENTENCE: 100\n",
      "Length of COMPLEX WORD COUNT: 100\n",
      "Length of WORD COUNT: 100\n",
      "Length of SYLLABLE PER WORD: 100\n",
      "Length of PERSONAL PRONOUNS: 100\n",
      "Length of AVG WORD LENGTH: 100\n"
     ]
    }
   ],
   "source": [
    "output_dict ={\n",
    "    'URL_ID':input_df['URL_ID'],\n",
    "    'URL':input_df['URL'],\n",
    "    'POSITIVE SCORE':[],\n",
    "    'NEGATIVE SCORE':[],\n",
    "    'POLARITY SCORE':[],\n",
    "    'SUBJECTIVITY SCORE':[],\n",
    "    'AVG SENTENCE LENGTH':[],\n",
    "    'PERCENTAGE OF COMPLEX WORDS':[],\n",
    "    'FOG INDEX':[],\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE':[],\n",
    "    'COMPLEX WORD COUNT':[],\n",
    "    'WORD COUNT':[],\n",
    "    'SYLLABLE PER WORD':[],\n",
    "    'PERSONAL PRONOUNS':[],\n",
    "    'AVG WORD LENGTH':[]\n",
    "}\n",
    "\n",
    "for c in content_list:\n",
    "\n",
    "    output_dict['POSITIVE SCORE'].append(c['Positive Score'])\n",
    "    output_dict['NEGATIVE SCORE'].append(c['Negative Score'])\n",
    "    output_dict['POLARITY SCORE'].append(c['Polarity Score'])\n",
    "    output_dict['SUBJECTIVITY SCORE'].append(c['Subjectivity Score'])\n",
    "    output_dict['AVG SENTENCE LENGTH'].append(c['Average Sentence Length'])\n",
    "    output_dict['PERCENTAGE OF COMPLEX WORDS'].append(c['Percentage of complex words'])\n",
    "    output_dict['FOG INDEX'].append(c['Fog index'])\n",
    "    output_dict['AVG NUMBER OF WORDS PER SENTENCE'].append(c['Average Number of Words Per '])\n",
    "    output_dict['COMPLEX WORD COUNT'].append(c['Complex Word Count'])\n",
    "    output_dict['WORD COUNT'].append(c['Word Count'])\n",
    "    output_dict['SYLLABLE PER WORD'].append(c['Syllable Count per word'])\n",
    "    output_dict['PERSONAL PRONOUNS'].append(c['Personal Pronouns'])\n",
    "    output_dict['AVG WORD LENGTH'].append(c['Average Word Length'])\n",
    "\n",
    "print(\"Length of URL_ID:\", len(output_dict['URL_ID']))\n",
    "print(\"Length of URL:\", len(output_dict['URL']))\n",
    "print(\"Length of POSITIVE SCORE:\", len(output_dict['POSITIVE SCORE']))\n",
    "print(\"Length of NEGATIVE SCORE:\", len(output_dict['NEGATIVE SCORE']))\n",
    "print(\"Length of POLARITY SCORE:\", len(output_dict['POLARITY SCORE']))\n",
    "print(\"Length of SUBJECTIVITY SCORE:\", len(output_dict['SUBJECTIVITY SCORE']))\n",
    "print(\"Length of AVG SENTENCE LENGTH:\", len(output_dict['AVG SENTENCE LENGTH']))\n",
    "print(\"Length of PERCENTAGE OF COMPLEX WORDS:\", len(output_dict['PERCENTAGE OF COMPLEX WORDS']))\n",
    "print(\"Length of FOG INDEX:\", len(output_dict['FOG INDEX']))\n",
    "print(\"Length of AVG NUMBER OF WORDS PER SENTENCE:\", len(output_dict['AVG NUMBER OF WORDS PER SENTENCE']))\n",
    "print(\"Length of COMPLEX WORD COUNT:\", len(output_dict['COMPLEX WORD COUNT']))\n",
    "print(\"Length of WORD COUNT:\", len(output_dict['WORD COUNT']))\n",
    "print(\"Length of SYLLABLE PER WORD:\", len(output_dict['SYLLABLE PER WORD']))\n",
    "print(\"Length of PERSONAL PRONOUNS:\", len(output_dict['PERSONAL PRONOUNS']))\n",
    "print(\"Length of AVG WORD LENGTH:\", len(output_dict['AVG WORD LENGTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been successfully saved to: Output Data Structures.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(output_dict)\n",
    "\n",
    "file_path = \"Output Data Structures.xlsx\"\n",
    "\n",
    "output_df.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame has been successfully saved to:\", file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
